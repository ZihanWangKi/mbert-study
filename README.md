# mBERT-Study
<h5 align="center">CROSS-LINGUAL ABILITY OF MULTILINGUAL BERT: AN EMPIRICAL STUDY</h5>

## Motivation 

[Multilingual
BERT](https://github.com/google-research/bert/blob/master/multilingual.md) (M-BERT) has shown surprising cross lingual abilities --- even when it is trained without cross lingual objectives.
In our work, we analyze this result from three factors: linguistic properties of the languages, the architecture
of the model, and the learning objectives.

## Results

TODO

## Scripts

#### Creating pre-training data

See [preprocessing-scripts](preprocessing-scripts)

#### Pre-training BERT

See [bert-running-scripts](bert-running-scripts).

#### Evaluating

See [evaluating-scripts](evaluating-scripts).
 

## Citation
Please cite the following paper if you find our paper useful. Thanks!

>Karthikeyan K, Zihan Wang, Stephen Mayhew, Dan Roth. "Cross-Lingual Ability of Multilingual BERT: An Empirical Study" arXiv preprint arXiv:1912.07840 (2019).

```
@article{,
  title={Cross-Lingual Ability of Multilingual BERT: An Empirical Study},
  author={K, Karthikeyan and Wang, Zihan and Mayhew, Stephen and Roth, Dan},
  journal={arXiv preprint arXiv:1912.07840},
  year={2019}
}
```
